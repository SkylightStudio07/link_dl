{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30a3cb7a-4416-456a-bcf7-794d34e58d65",
   "metadata": {},
   "source": [
    "# Assignment 1_1 - 딥러닝 및 실습.\n",
    "## 2020136052 컴퓨터공학부 - 박영서"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e6df65-7d5e-40be-8ec8-471cc5bac02b",
   "metadata": {},
   "source": [
    "### a_tensor_initialization.py\n",
    "\n",
    "텐서 관련 기본적인 메서드 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aee4e4e2-6a2b-4ad5-ac50-7680013d1902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.int64\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "#####\n",
      "torch.Size([]) 0\n",
      "torch.Size([1]) 1\n",
      "torch.Size([5]) 1\n",
      "torch.Size([5, 1]) 2\n",
      "torch.Size([3, 2]) 2\n",
      "torch.Size([3, 2, 1]) 3\n",
      "torch.Size([3, 1, 2, 1]) 4\n",
      "torch.Size([3, 1, 2, 3]) 4\n",
      "torch.Size([3, 1, 2, 3, 1]) 5\n",
      "torch.Size([4, 5]) 2\n",
      "torch.Size([4, 1, 5]) 3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.Tensor([1, 2, 3], device='cpu') # [1.. 2., 3.,] float 텐서 생성\n",
    "print(t1.dtype) # 텐서의 데이터 타입 확인 (float32)\n",
    "print(t1.device) # 텐서가 올려진 디바이스. 아까 CPU로 선언했다.\n",
    "print(t1.requires_grad) # 자동미분 추적 여부. 디폴트는 false.\n",
    "print(t1.size()) # 텐서의 차원 크기 반환. (torch.Size([3]))\n",
    "print(t1.shape) # 둘은 같은 함수다.\n",
    "t1_cpu = t1.cpu() # 이 텐서를 CPU 메모리에 있는 복사본으로 반환하는 것. 그러니까 새로운 텐서 객체다.\n",
    "# 여기서 t1_cpu는 새로운 텐서 타입 인스턴스로 선언된 것.\n",
    "\n",
    "t2 = torch.tensor([1, 2, 3]) # 팩토리 함수. 입력 타입을 자동으로 추론.\n",
    "# [1, 2, 3]은 int32, [1., 2., 3.] -> float32 \n",
    "# Tensor은 무조건 기본 타입(float32로 생성)\n",
    "print(t2.dtype)\n",
    "print(t2.device)\n",
    "print(t2.requires_grad)\n",
    "print(t2.size())\n",
    "print(t2.shape)\n",
    "\n",
    "t2_cpu = t2.cpu()\n",
    "\n",
    "\n",
    "print(\"#####\")\n",
    "\n",
    "\n",
    "a1=torch.tensor(1) # 스칼라(0차원) 텐서. ndim은 차원 수.\n",
    "print(a1.shape, a1.ndim)\n",
    "\n",
    "a2=torch.tensor([1]) # 길이 1의 1차원 벡터.\n",
    "print(a2.shape, a2.ndim)\n",
    "\n",
    "a3=torch.tensor([1, 2, 3, 4, 5])  \n",
    "print(a3.shape, a3.ndim)\n",
    "# 길이 5의 1차원 벡터\n",
    "\n",
    "a4=torch.tensor([[1], [2], [3], [4], [5]])   \n",
    "print(a4.shape, a4.ndim)\n",
    "# 2차원 행렬(5행 1열)\n",
    "\n",
    "a5 = torch.tensor([           # shape: torch.Size([3, 2]) 2차원 텐서\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [5, 6]\n",
    "])\n",
    "print(a5.shape, a5.ndim)\n",
    "\n",
    "a6 = torch.tensor([                 # shape: torch.Size([3, 2, 1]) - 3차원 텐서\n",
    "    [[1], [2]],\n",
    "    [[3], [4]],\n",
    "    [[5], [6]]\n",
    "])\n",
    "print(a6.shape, a6.ndim)\n",
    "\n",
    "a7 = torch.tensor([               # shape: torch.Size([3, 1, 2, 1]) - 4차원 텐서\n",
    "    [[[1], [2]]],\n",
    "    [[[3], [4]]],\n",
    "    [[[5], [6]]]\n",
    "])\n",
    "print(a7.shape, a7.ndim)\n",
    "\n",
    "a8 = torch.tensor([               # shape: torch.Size([3, 1, 2, 3]) - 4차원 텐서\n",
    "    [[[1, 2, 3], [2, 3, 4]]],\n",
    "    [[[3, 1, 1], [4, 4, 5]]],\n",
    "    [[[5, 6, 2], [6, 3, 1]]]\n",
    "])\n",
    "print(a8.shape, a8.ndim)\n",
    "\n",
    "\n",
    "a9 = torch.tensor([          # shape: torch.Size([3, 1, 2, 3, 1]) - 5차원 텐서\n",
    "    [[[[1], [2], [3]], [[2], [3], [4]]]],\n",
    "    [[[[3], [1], [1]], [[4], [4], [5]]]],\n",
    "    [[[[5], [6], [2]], [[6], [3], [1]]]]\n",
    "])\n",
    "print(a9.shape, a9.ndim)\n",
    "\n",
    "a10 = torch.tensor([             # shape: torch.Size([4, 5]) - 2차원 텐서    \n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "])\n",
    "print(a10.shape, a10.ndim)\n",
    "\n",
    "a10 = torch.tensor([             # shape: torch.Size([4, 4, 5]) - 3차원 텐서  \n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "])\n",
    "print(a10.shape, a10.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9321495-ffc1-4ae9-8b62-8f4e15ecd155",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 3 at dim 3 (got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m a11 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m                \u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 3 at dim 3 (got 2)"
     ]
    }
   ],
   "source": [
    "a11 = torch.tensor([                \n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a3e9f1-6fa1-4ba7-96a2-5ee41995b8d8",
   "metadata": {},
   "source": [
    "##### 오류가 발생하는 것이 정상. 모든 차원에서 크기가 일정해야 한다. 크기가 같았다고 가정하면 torch.size([4, 1, 2, 3]) 내지는 torch.size([4, 1, 2, 2])일 것."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070375db-68bf-451f-bb2c-f087e34e4c1e",
   "metadata": {},
   "source": [
    "## b_tensor_initialization_copy\n",
    "### 추가 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6e52a6e-0899-46f6-b345-223cd9c5c252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3])\n",
      "tensor([1, 2, 3])\n",
      "#######\n",
      "tensor([1., 2., 3.])\n",
      "tensor([1., 2., 3.])\n",
      "tensor([100,   2,   3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "l1 = [1, 2, 3]\n",
    "t1 = torch.Tensor(l1) ## 이 경우 주어진 데이터는 float32 텐서로 변환한닫.\n",
    "## Tensor로 텐서 인스턴스를 선언하면 원본 리스트와는 메모리를 공유하지 않음. -> 새로운 텐서를 생성.\n",
    "\n",
    "l2 = [1, 2, 3]\n",
    "t2 = torch.tensor(l2) ## 팩토리 함수기 때문에 인자 타입을 자동으로 추론한다. 따라서 텐서의 타입은 int64.\n",
    "## 새로운 텐서 생성.\n",
    "\n",
    "l3 = [1, 2, 3] # 리스트 생성\n",
    "t3 = torch.as_tensor(l3)\n",
    "## torch.as_tensor로 선언시 일부 경우에 따라 메모리를 공유하긴 하지만. 리스트는 메모리 공유가 불가하므로 새로운 텐서 인스턴스가 생성된다.\n",
    "\n",
    "l1[0] = 100\n",
    "l2[0] = 100\n",
    "l3[0] = 100\n",
    "\n",
    "print(t1)\n",
    "print(t2)\n",
    "print(t3)\n",
    "\n",
    "## 전부 결과값은 1, 2, 3으로 통일된다. torch.as.tensor로 선언한 l3이 있긴 하지만 리스트는 앞서 말했듯 메모리 공유가 안 되니까!\n",
    "\n",
    "\n",
    "print(\"#######\")\n",
    "\n",
    "\n",
    "l4 = np.array([1, 2, 3]) \n",
    "t4 = torch.Tensor(l4)\n",
    "## 넘파이 배열을 float32 텐서로 변환하여 새로운 텐서 생성. 메모리는 공유하지 않는다.\n",
    "\n",
    "l5 = np.array([1, 2, 3])\n",
    "t5 = torch.Tensor(l5)\n",
    "## 넘파이 배열을 int32 텐서로 변환하여 새로운 텐서 생성. 상기와 같이 메모리는 공유하지 않는다.\n",
    "\n",
    "l6 = np.array([1, 2, 3])\n",
    "t6 = torch.as_tensor(l6)\n",
    "## 이 경우에는 넘파이 배열와 메모리를 공유해서 얉은 복사의 형식으로 이루어진다. \n",
    "## 따라서 원본 넘파이 배열 값이 변경되면 텐서 값도 같이 변경된다.\n",
    "\n",
    "l4[0] = 100\n",
    "l5[0] = 100\n",
    "l6[0] = 100\n",
    "\n",
    "print(t4)\n",
    "print(t5)\n",
    "print(t6)\n",
    "\n",
    "## 이 경우에는 numpy 배열과 값을 공유중인 t6의 값만 변동할 것.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bae967-f353-455e-8cac-67c2c8661e1f",
   "metadata": {},
   "source": [
    "## c_tensor_initialization_constant_values\n",
    "#### 텐서 생성 및 초기화 관련 여러가지 메소드들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ed4565b-3105-4b81-a7df-6ba221544b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "#####\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "######\n",
      "tensor([1.0000e+00, 1.4013e-45, 0.0000e+00, 0.0000e+00])\n",
      "tensor([-5.9517e+20,  2.0809e-42,  0.0000e+00,  0.0000e+00])\n",
      "#####\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "## torch.ones() / torch.ones_like()\n",
    "\n",
    "t1 = torch.ones(size=(5, )) # shape (5,)짜리, 1로 채워진 텐서 생성. 기본 타입은 float32.\n",
    "t1_like = torch.ones_like(t1) ## t1와 같은 shape, 데이터 타입, 디바이스를 가지는 1로 태워진 텐서.\n",
    "\n",
    "print(t1)\n",
    "print(t1_like)\n",
    "\n",
    "print(\"#####\")\n",
    "\n",
    "## torch.zeros(), torch.zeros_like()\n",
    "\n",
    "t2 = torch.zeros(size=(6,)) # torch.ones()와 비슷하지만 이 쪽은 0으로 채운다.\n",
    "t2_like = torch.zeros_like(input=t2) ## 이쪽도 0으로 채움.\n",
    "\n",
    "print(t2)\n",
    "print(t2_like)\n",
    "\n",
    "print(\"######\")\n",
    "\n",
    "### torch.empty() / torch.empty_like()\n",
    "\n",
    "t3 = torch.empty(size=(4,)) ## shape(4,)짜리 '초기화되지 않은' 텐서 생성.\n",
    "t3_like = torch.empty_like(input=t3)\n",
    "\n",
    "print(t3) ## 이 경우 메모리에 남아있는 임의의 값이나 0일수도 있는 것. 초기화되지 않은 거니까...\n",
    "print(t3_like)\n",
    "\n",
    "print(\"#####\")\n",
    "\n",
    "### torch.eye\n",
    "\n",
    "t4 = torch.eye(n=3) # 3x3 단위행렬 생성. 대각선만 1인 행렬이다.\n",
    "print(t4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cd348c-95b6-4875-8318-09fa272884ad",
   "metadata": {},
   "source": [
    "## d_tensor_initialization_random_values\n",
    "#### 난수 / 수열 생성 메소드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64f2ef2b-3a7e-4f77-8156-3c9adac3915e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[17, 12]])\n",
      "tensor([[0.0453, 0.5035, 0.9978]])\n",
      "tensor([0.0000, 2.5000, 5.0000])\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "######\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n",
      "\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.randint(low=10, high=20, size=(1, 2))\n",
    "## shape가 (1, 2)인 텐서 생성, 여기서 내부 값은 10 이상 20 미만(정수)\n",
    "print(t1)\n",
    "\n",
    "t2 = torch.rand(size=(1, 3))\n",
    "## 이 경우엔 [0.0, 1.0) 범위의 균일분포 난수. shape는 (1,3)\n",
    "print(t2)\n",
    "\n",
    "t3=torch.randn(size=(1, 3))\n",
    "## 평균 0, 표준편차 1의 정규분포 난수. shape는 (1, 3)\n",
    "\n",
    "t4 = torch.normal(mean = 10.0, std = 1.0, size = (3, 2))\n",
    "## 평균 10.0, 표준편자 1.0의 정규분포 난수. shape=(3, 2)\n",
    "\n",
    "t5 = torch.linspace(start = 0.0, end = 5.0, steps = 3)\n",
    "## 0.0부터 5.0까지 균등 간격으로 3개 값 생성. [0.0, 5.0]\n",
    "print(t5)\n",
    "\n",
    "t6 = torch.arange(5)\n",
    "# 0부터 4까지 정수 시퀀스 생성(range 유사)\n",
    "# -> [0, 1, 2, 3, 4]\n",
    "print(t6)\n",
    "\n",
    "print(\"######\")\n",
    "\n",
    "# 여기서부터는 시드 난수 관련\n",
    "\n",
    "torch.manual_seed(1729) # 시드 고정. 이러면 이후 난수값이 항상 동일하다. 물론 시드값을 기억하고 있다는 가정 하에...\n",
    "\n",
    "random1 = torch.rand(2,3)\n",
    "print(random1) # seed : 1729\n",
    "random2 = torch.rand(2, 3)\n",
    "print(random2) \n",
    "\n",
    "print()\n",
    "\n",
    "torch.manual_seed(1729) # 같은 시드로 초기화\n",
    "\n",
    "random3 = torch.rand(2, 3) # random1과 같은 값.\n",
    "print(random3)\n",
    "random4 = torch.rand(2, 3)\n",
    "print(random4) # random2와 같은 값."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84007f0-012a-4117-912c-47b9b82a61e4",
   "metadata": {},
   "source": [
    "## e_tensor_type_conversion\n",
    "#### 텐서 자료형 지정 및 변환 관련 메소드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a557c9ac-45ee-402d-a8ba-fa06f0eb3db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.int16\n",
      "tensor([[7.6611e+00, 1.8801e+01, 3.9405e+00, 1.0331e+01, 8.9673e+00, 4.9067e+00,\n",
      "         9.1012e-03, 5.9629e+00, 1.1071e+01, 1.2571e+01, 1.6582e+01, 1.8456e+01,\n",
      "         1.8698e-02, 8.1616e+00, 1.3194e+01, 7.2011e+00, 1.7360e+01, 4.2060e+00,\n",
      "         1.2422e+01, 1.1559e+00, 6.5834e+00, 4.0778e+00, 1.4803e+01, 1.4196e+01,\n",
      "         1.5153e+01, 1.8894e+01, 1.1434e+01, 1.9444e+01, 1.6538e+01, 8.7672e+00],\n",
      "        [9.1008e+00, 1.9145e+01, 1.3189e+01, 3.0974e+00, 1.0855e+01, 1.7935e+01,\n",
      "         8.3659e+00, 1.1664e+00, 1.2513e+01, 1.2812e+01, 8.9558e+00, 4.2822e+00,\n",
      "         1.0630e+01, 9.2623e+00, 7.9516e+00, 1.6139e+01, 9.3634e+00, 4.7077e+00,\n",
      "         2.2244e+00, 9.7197e+00, 1.8577e+01, 1.7785e+01, 9.5398e+00, 1.3406e+01,\n",
      "         1.0089e+01, 3.0597e+00, 9.7314e+00, 1.5772e+01, 1.2001e+01, 1.4104e+01]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int32)\n",
      "tensor([[1, 2]], dtype=torch.int16)\n",
      "torch.float64\n",
      "torch.int16\n",
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.ones((2, 3))\n",
    "print(a.dtype) ## 디폴트는 torch.float32다.\n",
    "\n",
    "b = torch.ones((2, 3), dtype = torch.int16) ## dtype에 인자를 넣어 직접 자료형 지정 가능\n",
    "print(b.dtype)\n",
    "\n",
    "c = torch.rand((2, 30), dtype = torch.float64) * 20 ## 여기서 * 20은 스칼라 곱. 모든 원소에 20을 곱한다.\n",
    "## [0.0, 1.0) -> [0.0, 20.0)\n",
    "print(c)\n",
    "\n",
    "### 여기서부터는 기존 텐서의 dtype을 변환하는 메소드들.\n",
    "\n",
    "d = b.to(torch.int32)\n",
    "print(d)\n",
    "\n",
    "### 다양한 방식으로 데이터 타입 변환\n",
    "\n",
    "## 생성시 지정\n",
    "double_d = torch.ones(10, 2, dtype = torch.double)\n",
    "# tensor([[1., 1.],\n",
    "#         [1., 1.],\n",
    "#         ...\n",
    "#         [1., 1.]], dtype=torch.float64)\n",
    "short_e = torch.tensor([[1, 2]], dtype = torch.short) \n",
    "print(short_e)\n",
    "\n",
    "double_d = torch.zeros(10, 2).to(torch.double)\n",
    "short_3 = torch.ones(10, 2).to(dtype=torch.short)\n",
    "\n",
    "double_d = torch.zeros(10, 2).type(torch.double)\n",
    "short_e = torch.ones(10, 2).to(dtype=torch.short)\n",
    "\n",
    "print(double_d.dtype) # double = float64이므로 float64 출력\n",
    "print(short_e.dtype) # torch.int16 -> short\n",
    "\n",
    "## 연산시 dtype 브로드캐스팅\n",
    "\n",
    "double_f = torch.rand(5, dtype=torch.double) #float64\n",
    "short_g = double_f.to(torch.short)\n",
    "\n",
    "print((double_f * short_g).dtype)\n",
    "# float64 * int16 -> int16이 float64로 승격되면서 float64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057c2e40-9278-4a27-8e56-07afc59efcfa",
   "metadata": {},
   "source": [
    "## f_tensor_operations\n",
    "#### 파이토치 산술연산함수 VS 연산자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4442151-b0a7-439b-a3b7-5c9beff99b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "#####\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "######\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "######\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "### 덧셈\n",
    "\n",
    "t1 = torch.ones(size=(2, 3))\n",
    "#[[1, 1, 1], [1, 1, 1]]\n",
    "t2= torch.ones(size=(2, 3))\n",
    "\n",
    "t3 = torch.add(t1, t2) # -> 함수 방식\n",
    "t4 = t1 + t2 # -> 연산자 오버로딩\n",
    "print(t3)\n",
    "print(t4)\n",
    "\n",
    "### 뺄셈\n",
    "print(\"#####\")\n",
    "\n",
    "t5 = torch.sub(t1, t2)\n",
    "t6 = t1 - t2\n",
    "\n",
    "print(t5)\n",
    "print(t6)\n",
    "\n",
    "print(\"######\")\n",
    "\n",
    "### 곱셈\n",
    "\n",
    "t7 = torch.mul(t1, t2)\n",
    "t8 = t1 * t2\n",
    "print(t7)\n",
    "print(t8)\n",
    "\n",
    "## 나눗셈\n",
    "print(\"######\")\n",
    "\n",
    "t9 = torch.div(t1, t2)\n",
    "t10 = t1 / t2\n",
    "print(t9)\n",
    "print(t10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cbeba0-8a29-4342-9147-4dc238f3cde7",
   "metadata": {},
   "source": [
    "#### 모두 Element-Wise 방식--즉 같은 위치의 연산끼리 계산한다.\n",
    "#### 기본적으로 shape가 같아야 하며, 다를 경우 브로드캐스팅 규칙이 적용된다. (작은 쪽의 텐서가 늘어남)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5604b9a8-030a-4f6b-a005-d6e3b45596a5",
   "metadata": {},
   "source": [
    "## g_tensor_operations_mm\n",
    "#### 내적, 행렬곱, 배치 행렬곱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "740195c2-5bfa-49e4-afd4-7b9bd92584d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7) torch.Size([])\n",
      "######\n",
      "tensor([[ 0.9053,  0.3740],\n",
      "        [ 0.2944, -0.0980]]) torch.Size([3, 2])\n",
      "######\n",
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.dot(\n",
    "    torch.tensor([2, 3]), # shape -> 2\n",
    "    torch.tensor([2, 1]) # shape -> 2\n",
    ")\n",
    "print(t1, t1.size())\n",
    "# t1 = 2 * 2 + 3 * 1 = 7, 내적값은 스칼라다.\n",
    "\n",
    "print(\"######\")\n",
    "### 2D 행렬곱\n",
    "\n",
    "t2 = torch.randn(2, 3)\n",
    "t3 = torch.randn(3, 2)\n",
    "t4 = torch.mm(t2, t3)\n",
    "print(t4, t3.size())\n",
    "\n",
    "print(\"######\")\n",
    "### 3D 배치 행렬 곱셈\n",
    "\n",
    "t5 = torch.randn(10, 3, 4)  # 10개의 (3x4) 행렬\n",
    "t6 = torch.randn(10, 4, 5)  # 10개의 (4x5) 행렬\n",
    "t7 = torch.bmm(t5, t6)\n",
    "print(t7.size())\n",
    "# 배치 10개 * (A의 행 × A의 열) @ (B의 행 × B의 열) → (A의 행 × B의 열)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f498f1e-4979-4299-8057-0fb216188249",
   "metadata": {},
   "source": [
    "## h_tensor_operations_matmul\n",
    "torch.matmul -> 차원에 따른 내적, 행렬곱, 3D 배치 행렬곱 동작 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3047a07f-7636-423c-ae1b-72d26bf487ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "torch.Size([3])\n",
      "torch.Size([10, 3])\n",
      "torch.Size([10, 3, 5])\n",
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "## 벡터 x 벡터 -> 내적\n",
    "\n",
    "t1 = torch.randn(3)\n",
    "t2 = torch.randn(3) \n",
    "print(torch.matmul(t1, t2).size()) ## 어차피 스칼라니 0차원 스칼라 텐서...\n",
    "\n",
    "## 행렬 x 벡터 -> 행렬곱\n",
    "\n",
    "t3 = torch.rand(3, 4)\n",
    "t4 = torch.rand(4)\n",
    "print(torch.matmul(t3, t4).size()) ## 행렬 x 벡터시 벡터이므로 일차원 벡터 크기. 순서가 바뀌어도 알아서 matmul()이 보정해 줌\n",
    "\n",
    "## 배치별 행렬곱\n",
    "\n",
    "t5 = torch.randn(10, 3, 4) # shape [10, 3, 4]\n",
    "t6 = torch.rand(4) # 스칼라\n",
    "print(torch.matmul(t5, t6).size()) \n",
    "# 배치별 행렬곱. 마지막 두 차원만 행렬곱으로 계산하고 그 앞 차원은 전부 브로드캐스팅 차원\n",
    "\n",
    "t7 = torch.randn(10, 3, 4)  # [10,3,4]\n",
    "t8 = torch.randn(10, 4, 5)  # [10,4,5]\n",
    "print(torch.matmul(t7, t8).size())  \n",
    "# → torch.Size([10, 3, 5])\n",
    "\n",
    "t9 = torch.randn(10, 3, 4)  # [10,3,4]\n",
    "t10 = torch.randn(4, 5)     # [4,5]\n",
    "print(torch.matmul(t9, t10).size())  \n",
    "# → torch.Size([10, 3, 5]) $ (4, 5)가 (10, 4, 5)로 자동 브로드캐스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd04946-bb37-4a1a-b73e-a9bb993a08ba",
   "metadata": {},
   "source": [
    "## i_tensor_broadcasting\n",
    "거듭제곱, 브로드캐스팅, 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69e74809-a450-4a96-9a32-f19142454798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4., 6.])\n",
      "######\n",
      "tensor([[-4, -4],\n",
      "        [-2, -1],\n",
      "        [ 6,  5]])\n",
      "######\n",
      "torch.Size([3, 28, 28])\n",
      "######\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([5, 3, 4, 1])\n",
      "######\n",
      "torch.Size([5, 3, 4, 1])\n",
      "torch.Size([3, 1, 7])\n",
      "torch.Size([3, 3, 3])\n",
      "######\n",
      "tensor([5., 5., 5., 5.])\n",
      "tensor([25., 25., 25., 25.])\n",
      "tensor([  1.,   4.,  27., 256.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "t2 = 2.0\n",
    "print(t1 * t2)\n",
    "\n",
    "print(\"######\")\n",
    "### 2차원 - 1차원 브로드캐스팅\n",
    "\n",
    "t3 = torch.tensor([[0, 1], [2, 4], [10, 10]])\n",
    "t4 = torch.tensor([4, 5])\n",
    "print (t3 - t4)\n",
    "\n",
    "print(\"######\")\n",
    "### 정규화\n",
    "\n",
    "def normalize(x) :\n",
    "    return x / 255 # 이미지 픽셀 값은 보통 0~255이므로... 0~1 사이의 실수로 바꾸는 정규화를 수행하는 것.\n",
    "\n",
    "t6 = torch.randn(3, 28, 28)\n",
    "print(normalize(t6).size()) ## 단순 정규화이므로 크기는 유지.\n",
    "\n",
    "print(\"######\")\n",
    "### 정규화 예제\n",
    "\n",
    "t11 = torch.ones(4, 3, 2)               \n",
    "t12 = t11 * torch.rand(3, 2)              \n",
    "print(t12.shape)  # torch.Size([4,3,2])\n",
    "\n",
    "t13 = torch.ones(4, 3, 2)\n",
    "t14 = t13 * torch.rand(3, 1)               \n",
    "print(t14.shape)\n",
    "\n",
    "t15 = torch.ones(4, 3, 2)\n",
    "t16 = t15 * torch.rand(1, 2)               \n",
    "print(t16.shape)\n",
    "\n",
    "t17 = torch.ones(5, 3, 4, 1)               \n",
    "t18 = torch.rand(3, 1, 1)                  \n",
    "print((t17 + t18).size())\n",
    "\n",
    "## 배치 차원 확장용으로 왼쪽에 1 하나 늘리는 건 아무런 문제 없음!\n",
    "## 같은 차원이고, 원소가 다르면 (값이 1인 축만) 같은 값으로 확장.\n",
    "\n",
    "print(\"######\")\n",
    "\n",
    "t19 = torch.empty(5, 1, 4, 1)\n",
    "t20 = torch.empty(3, 1, 1)\n",
    "print((t19 + t20).size())  # torch.Size([5, 3, 4, 1])\n",
    "\n",
    "t21 = torch.empty(1)\n",
    "t22 = torch.empty(3, 1, 7)\n",
    "print((t21 + t22).size())  # torch.Size([3, 1, 7])\n",
    "\n",
    "t23 = torch.ones(3, 3, 3)\n",
    "t24 = torch.ones(3, 1, 3)\n",
    "print((t23 + t24).size())  # torch.Size([3, 3, 3])\n",
    "\n",
    "t25 = torch.empty(5, 2, 4, 1)\n",
    "t26 = torch.empty(3, 1, 1)\n",
    "\n",
    "# print((t25 + t26).size())\n",
    "# RuntimeError: The size of tensor a (2) must match\n",
    "\n",
    "## 축 확장은 어디까지나 1인 축만 가능한 것. 2 vs 3이라 아예 호환 불가!\n",
    "\n",
    "print(\"######\")\n",
    "### 거듭제곱\n",
    "\n",
    "t27 = torch.ones(4) * 5\n",
    "print(t27)  # tensor([5.,5.,5.,5.])\n",
    "\n",
    "t28 = torch.pow(t27, 2)  \n",
    "print(t28)  # tensor([25.,25.,25.,25.])\n",
    "\n",
    "exp = torch.arange(1., 5.) \n",
    "a = torch.arange(1., 5.)    # 지수\n",
    "t29 = torch.pow(a, exp)\n",
    "print(t29)  # tensor([1., 4., 27., 256.])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7762ec7-fe9e-4dde-8438-0649676c7f0b",
   "metadata": {},
   "source": [
    "## j_tensor_indexing_slicing\n",
    "텐서 인덱싱 / 슬라이싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faecc3f9-9760-4358-afdb-cf9a079dc00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 6, 7, 8, 9])\n",
      "tensor([ 1,  6, 11])\n",
      "tensor(7)\n",
      "tensor([ 4,  9, 14])\n",
      "######\n",
      "tensor([[ 5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14]])\n",
      "tensor([[ 8,  9],\n",
      "        [13, 14]])\n",
      "######\n",
      "tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]])\n",
      "######\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 3, 4, 5]])\n",
      "tensor([[3, 4],\n",
      "        [6, 7]])\n",
      "tensor([[2, 3, 4],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 0, 0, 5],\n",
      "        [5, 0, 0, 8]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "## 기본 인덱싱\n",
    "\n",
    "x = torch.tensor(\n",
    "  [[0, 1, 2, 3, 4],\n",
    "   [5, 6, 7, 8, 9],\n",
    "   [10, 11, 12, 13, 14]]\n",
    ")\n",
    "\n",
    "print(x[1]) ## 두 번째 행\n",
    "print(x[:, 1]) ## 모든 행의 두 번째 열\n",
    "print(x[1, 2]) ## 1행 2열 원소 -> 2\n",
    "print(x[:, -1]) ## 모든 행의 마지막 열. 이제 많이 쓰겠징\n",
    "\n",
    "\n",
    "print(\"######\")\n",
    "### 슬라이싱 예시\n",
    "\n",
    "print(x[1:]) ## 1행부터 끝까지 전부\n",
    "print(x[1:, 3:]) ## 1행 ~ / 3열 ~\n",
    "\n",
    "print(\"######\")\n",
    "### 부분선택 / 값 대입\n",
    "\n",
    "y = torch.zeros((6, 6))\n",
    "\n",
    "y[1:4, 2] = 1 # 1행에서 3열까지 2열 전부 1로\n",
    "print(y)\n",
    "\n",
    "print(y[1:4, 1:4]) ## 1행~3행, 1열~3열만 잘라서 출력\n",
    "\n",
    "print(\"######\")\n",
    "### 부분 선택 / 값 대입\n",
    "\n",
    "z = torch.tensor(\n",
    "  [[1, 2, 3, 4],\n",
    "   [2, 3, 4, 5],\n",
    "   [5, 6, 7, 8]]\n",
    ")\n",
    "\n",
    "print(z[:2])\n",
    "# 0~1행 전부\n",
    "\n",
    "print(z[1:, 1:3])\n",
    "# 1~2행, 1~2열\n",
    "\n",
    "print(z[:, 1:])\n",
    "# 모든 행, 1~3열 → [[2,3,4],[3,4,5],[6,7,8]]\n",
    "\n",
    "z[1:, 1:3] = 0\n",
    "# 1~2행, 1~2열 부분을 0으로 바꾸기\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84079be-e126-4ed9-b03a-ce34e108dd12",
   "metadata": {},
   "source": [
    "## k_tensor_reshaping\n",
    "텐서 셰이프 변환 관련 메소드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b12921e2-6b04-40e1-8194-5127c7790cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "tensor([[1, 2, 3, 4, 5, 6]])\n",
      "tensor([[0, 1, 2, 3],\n",
      "        [4, 5, 6, 7]])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "#####\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[[1, 2, 3]],\n",
      "\n",
      "        [[4, 5, 6]]]) torch.Size([2, 1, 3])\n",
      "######\n",
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([1, 2, 3, 4, 5, 6, 7, 8])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n",
      "######\n",
      "torch.Size([2, 3, 5])\n",
      "torch.Size([5, 2, 3])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "#######\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t2 = t1.view(3, 2)\n",
    "t3 = t1.reshape(1, 6)\n",
    "\n",
    "print(t2)\n",
    "print(t3)\n",
    "\n",
    "## view VS reshape :\n",
    "## 원소 수가 같다는 전제 하에 둘다 셰이프만 바뀌지만, 메모리 처리 방식이 다름.\n",
    "## view는 연속된 메모리가 필요하고, reshape는 필요하면 복사도 함.\n",
    "\n",
    "t4 = torch.arange(8).view(2, 4)  # shape [2,4]\n",
    "t5 = torch.arange(6).view(2, 3)  # shape [2,3]\n",
    "print(t4)\n",
    "print(t5)\n",
    "\n",
    "print(\"#####\")\n",
    "### unsqueez -> 새 축 추가\n",
    "\n",
    "t9 = torch.tensor([1, 2, 3])\n",
    "t10 = t9.unsqueeze(1) # -> 1번 자리에 새 축 추가\n",
    "print(t10)\n",
    "t11 = torch.tensor([[1, 2, 3],\n",
    "                    [4, 5, 6]])  # shape [2,3]\n",
    "t12 = t11.unsqueeze(1)             # 1번 위치에 새 축 → [2,1,3]\n",
    "print(t12, t12.shape)\n",
    "\n",
    "print(\"######\")\n",
    "### flatten -> 1차원으로 펴기. 직관적이네\n",
    "\n",
    "t13 = torch.tensor([[1, 2, 3],\n",
    "                    [4, 5, 6]])  # shape [2,3]\n",
    "t14 = t13.flatten()  # [6]\n",
    "print(t14)  # [1,2,3,4,5,6]\n",
    "\n",
    "t15 = torch.tensor([[[1, 2],[3, 4]],\n",
    "                    [[5, 6],[7, 8]]])  # shape [2,2,2]\n",
    "t16 = torch.flatten(t15)\n",
    "t17 = torch.flatten(t15, start_dim=1)  ## 0번째 차원만 남기고 전부 평탄화.\n",
    "## [1, 2, 3, 4], [5, 6, 7, 8]\n",
    "\n",
    "print(t16)\n",
    "print(t17)\n",
    "\n",
    "print(\"######\")\n",
    "### 차원 순서 바꾸기 -> permute\n",
    "\n",
    "t18 = torch.randn(2, 3, 5)                 # shape [2,3,5]\n",
    "print(t18.shape)\n",
    "print(torch.permute(t18, (2, 0, 1)).size())  # shape [5,2,3]\n",
    "t19 = torch.tensor([[1, 2, 3],\n",
    "                    [4, 5, 6]])\n",
    "t20 = torch.permute(t19, dims=(0, 1))\n",
    "t21 = torch.permute(t19, dims=(0, 1))\n",
    "print(t20)\n",
    "print(t21)\n",
    "\n",
    "print(\"#######\")\n",
    "### transpose -> 전치\n",
    "\n",
    "t22 = torch.transpose(t19, 0, 1) # (행, 열) 교환 -> [3, 2]\n",
    "print(t22)\n",
    "\n",
    "t23 = torch.t(t19) # 2D 전용 전치 -> [3, 2]\n",
    "print(t23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d415bc8-c895-4d6f-933b-fc3ba1733472",
   "metadata": {},
   "source": [
    "## l_tensor_concat\n",
    "여러 텐서를 지정한 축을 따라 이어붙이는"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3a6d5b4-40fc-41a0-977d-d3e520a15d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n",
      "######\n",
      "torch.Size([8])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
      "######\n",
      "torch.Size([4, 3])\n",
      "torch.Size([2, 6])\n",
      "######\n",
      "torch.Size([6, 3])\n",
      "torch.Size([2, 9])\n",
      "######\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([1, 4, 3])\n",
      "torch.Size([1, 2, 6])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.zeros([2, 1, 3])\n",
    "t2 = torch.zeros([2, 3, 3])\n",
    "t3 = torch.zeros([2, 2, 3])\n",
    "\n",
    "t4 = torch.cat([t1, t2, t3], dim = 1)\n",
    "print(t4.shape) # torch.Size([2, 6, 3])\n",
    "### 1번 차원이 1+3+2 = 6으로 합쳐진 것.\n",
    "\n",
    "print(\"######\")\n",
    "\n",
    "t5 = torch.arange(0, 3) # -> shape[3]\n",
    "t6 = torch.arange(3, 8) # -> shape[5]\n",
    "\n",
    "t7 = torch.cat((t5, t6), dim = 0)\n",
    "print(t7.shape)\n",
    "print(t7)\n",
    "\n",
    "print(\"######\")\n",
    "### 2D 텐서끼리 이어붙이기 (dim = 0, dim = 1)\n",
    "\n",
    "t8 = torch.arange(0, 6).reshape(2, 3)\n",
    "t9 = torch.arange(6, 12).reshape(2, 3)\n",
    "\n",
    "t10 = torch.cat((t8, t8), dim = 0)\n",
    "print(t10.size()) # -> (4, 3)\n",
    "t11 = torch.cat((t8, t9), dim=1)\n",
    "print(t11.size())  # (2, 6)\n",
    "\n",
    "print(\"######\")\n",
    "### 여러 개 한 번에\n",
    "\n",
    "t12 = torch.arange(0, 6).reshape(2, 3)\n",
    "t13 = torch.arange(6, 12).reshape(2, 3)\n",
    "t14 = torch.arange(12, 18).reshape(2, 3)\n",
    "\n",
    "t15 = torch.cat((t12, t13, t14), dim=0)\n",
    "print(t15.size())  # [6, 3]\n",
    "\n",
    "t16 = torch.cat((t12, t13, t14), dim=1)\n",
    "print(t16.size())  # [2, 9]\n",
    "\n",
    "print(\"######\")\n",
    "### 3D 텐서\n",
    "\n",
    "t17 = torch.arange(0, 6).reshape(1, 2, 3)\n",
    "t18 = torch.arange(6, 12).reshape(1, 2, 3)\n",
    "\n",
    "t19 = torch.cat((t17, t18), dim = 0)\n",
    "print(t19.size()) # [2, 2, 3]\n",
    "\n",
    "t20 = torch.cat((t17, t18), dim = 1)\n",
    "print(t20.size()) # [1, 4, 3]\n",
    "\n",
    "t21 = torch.cat((t17, t18), dim = 2)\n",
    "print(t21.size())  # [1, 2, 6]\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "85068c77-17e5-4cbb-a684-d3d3697cfaa4",
   "metadata": {},
   "source": [
    "dim=0 (행 추가)\n",
    "[ a b ]      [ e f ]\n",
    "[ c d ]  +   [ g h ]   → [ a b ]\n",
    "                         [ c d ]\n",
    "                         [ e f ]\n",
    "                         [ g h ]\n",
    "\n",
    "dim=1 (열 추가)\n",
    "[ a b ]      [ e f ]\n",
    "[ c d ]  +   [ g h ]   → [ a b e f ]\n",
    "                         [ c d g h ]\n",
    "\n",
    "도식화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec1e876-5b3a-483f-9a56-d4f044273908",
   "metadata": {},
   "source": [
    "## m_tensor_stacking.py\n",
    "stack vs cat + unsqueeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "888e0fb1-8a2c-4b91-b4cb-31b858bbea5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3]) True\n",
      "torch.Size([2, 2, 3]) True\n",
      "torch.Size([2, 3, 2]) True\n",
      "#######\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "True\n",
      "#####\n",
      "tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t2 = torch.tensor([[7, 8, 9], [10, 11, 12]])   # shape [2,3]\n",
    "\n",
    "t3 = torch.stack([t1, t2], dim=0) \n",
    "# 새 축(dim = 0)을 만들고 그 아래에 두 텐서를 쌓는 것\n",
    "# shape [2, 2, 3]\n",
    "\n",
    "t4 = torch.cat([t1.unsqueeze(dim=0), t2.unsqueeze(dim=0)], dim=0)\n",
    "# → 둘 다 [1,2,3이므로 cat하면 [2,2,3]\n",
    "print(t3.shape, t3.equal(t4))  # torch.Size([2, 2, 3]) True 출력.\n",
    "\n",
    "t5 = torch.stack([t1, t2], dim=1)  \n",
    "t6 = torch.cat([t1.unsqueeze(dim=1), t2.unsqueeze(dim=1)], dim=1)\n",
    "\n",
    "print(t5.shape, t5.equal(t6))\n",
    "\n",
    "t7 = torch.stack([t1, t2], dim=2)\n",
    "t8 = torch.cat([t1.unsqueeze(dim=2), t2.unsqueeze(dim=2)], dim=2)\n",
    "print(t7.shape, t7.equal(t8)) ## 셋 다 true.\n",
    "# 두 방식의 차이점이라면 stack은 새 축을 만들고 그 아래로 텐서를 쌓는 거라면,\n",
    "# cat은 미리 새 축을 만든 뒤 거기다 이어붙이는 것. 어차피 결과는 같음.'\n",
    "\n",
    "print(\"#######\")\n",
    "## 2D로 비교시\n",
    "\n",
    "t9 = torch.arange(0, 3)\n",
    "t10 = torch.arange(3, 6)\n",
    "\n",
    "t11 = torch.stack((t9, t10), dim = 0)\n",
    "print(t11)\n",
    "\n",
    "t12 = torch.cat((t9.unsqueeze(0), t10.unsqueeze(0)), dim = 0)\n",
    "print(t11.equal(t12))\n",
    "\n",
    "print(\"#####\")\n",
    "### 1D -> 2D\n",
    "\n",
    "t13 = torch.stack((t9, t10), dim=1)  # 열을 추가한다는 감각으로\n",
    "print(t13)\n",
    "# [[0,3],\n",
    "#  [1,4],\n",
    "#  [2,5]]\n",
    "\n",
    "t14 = torch.cat((t9.unsqueeze(1), t10.unsqueeze(1)), dim=1)\n",
    "print(t13.equal(t14))  # True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3201437-1ebe-48f6-b688-71cfff867407",
   "metadata": {},
   "source": [
    "## n_tensor_vstack_hstack.py\n",
    "수직 - 수평으로 붙이기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05c30e2d-fcca-46b0-8a51-696b25f0ece6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "######\n",
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "torch.Size([2, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([1, 2, 3])\n",
    "t2 = torch.tensor([4, 5, 6])\n",
    "t3 = torch.vstack((t1, t2))\n",
    "print(t3) \n",
    "# [[1,2,3],\n",
    "#  [4,5,6]]\n",
    "# 수직으로 붙임\n",
    "# 1D 벡터는 2D화\n",
    "\n",
    "t4 = torch.tensor([[1], [2], [3]])\n",
    "t5 = torch.tensor([[4], [5], [6]])\n",
    "## 행이 3개, 열이 1개라는 감각으로\n",
    "\n",
    "t6 = torch.vstack((t4, t5))\n",
    "# [[1],[2],[3],[4],[5],[6]]\n",
    "# shape [6,1]\n",
    "\n",
    "t7 = torch.tensor([\n",
    "  [[1,2,3],[4,5,6]],\n",
    "  [[7,8,9],[10,11,12]]\n",
    "])  \n",
    "\n",
    "t8 = torch.tensor([\n",
    "  [[13,14,15],[16,17,18]],\n",
    "  [[19,20,21],[22,23,24]]\n",
    "])  # [2,2,3]\n",
    "\n",
    "t9 = torch.vstack([t7, t8])\n",
    "## 3D 텐서에서는  깊게 생각할 것 없이 vstack = torch.cat(..., dim = 0) -> [4, 2, 3]\n",
    "\n",
    "print(\"######\")\n",
    "### torch.hstack -> 수평 방향으로..\n",
    "\n",
    "t10 = torch.tensor([1,2,3])\n",
    "t11 = torch.tensor([4,5,6])\n",
    "t12 = torch.hstack((t10, t11))\n",
    "print(t12)\n",
    "# [1,2,3,4,5,6]\n",
    "\n",
    "## hstack에서 1D 벡터는 단순 이어붙이기.\n",
    "\n",
    "t13 = torch.tensor([[1],[2],[3]])  # shape [3,1]\n",
    "t14 = torch.tensor([[4],[5],[6]])  # shape [3,1]\n",
    "t15 = torch.hstack((t13, t14))\n",
    "print(t15)\n",
    "# [[1,4],\n",
    "#  [2,5],\n",
    "#  [3,6]]\n",
    "\n",
    "# 2D 세로벡터는 열이 늘어난다.(2D vstack() -> 행)\n",
    "\n",
    "t16 = torch.tensor([\n",
    "  [[1,2,3],[4,5,6]],\n",
    "  [[7,8,9],[10,11,12]]\n",
    "])  # [2,2,3]\n",
    "\n",
    "t17 = torch.tensor([\n",
    "  [[13,14,15],[16,17,18]],\n",
    "  [[19,20,21],[22,23,24]]\n",
    "])  # [2,2,3]\n",
    "\n",
    "t18 = torch.hstack([t16, t17])\n",
    "print(t18.shape)\n",
    "### 3D 텐서는 dim = 1로 이어붙이기."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adae8659-a13b-4a0e-97ec-9770053d7ef2",
   "metadata": {},
   "source": [
    "# 숙제 후기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad884b7c-de80-49a2-b529-89c68aae3711",
   "metadata": {},
   "source": [
    "_02_tensors에서는 텐서의 기본적인 메소드들을 학습하였다.  \n",
    "텐서의 메소드들 자체는 직관적으로 짜여있어 사용하는 데에 문제는 없었지만, 2차원~3차원 행렬을 감각적으로 떠올리면서 이해하면서 진행하는 경우가 많아 어느정도 시간이 걸렸다.  \n",
    "다만 브로드캐스팅으로 차원을 늘리거나 하는 것이 처음에는 직관적으로 이해가 가질 않아 생각보다 시간을 까먹었으며, 행렬 곱은 군생활 전 1학년때나 하던 거라 순간적으로 기억이 나질 않아 구글 등지에서 다시 한 번 찾아봐야 했다.   \n",
    "아직도 순간순간 단순 텐서 크기와 차원이 헷갈릴 때가 있는데, 이건 계속 텐서에 익숙해지면 빨리 해결되리라 믿어야 할 것 같다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:link_dl]",
   "language": "python",
   "name": "conda-env-link_dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
